{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning for Text Classification"
      ],
      "metadata": {
        "id": "UR87YzWqID2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget==3.2\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKBYTjPCIaBs",
        "outputId": "921bfadf-eb8a-4df7-b075-bfb62dc3e772"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=a091cc079cd342c3bada88393ecd9414a76a1754360c2a5b45dd2d8093242616\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E0NBSkvFHs5-"
      },
      "outputs": [],
      "source": [
        "#Make the necessary imports\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import wget\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from zipfile import ZipFile\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100"
      ],
      "metadata": {
        "id": "D3suITUDJ_YL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and Preprocessing"
      ],
      "metadata": {
        "id": "HciOSX8jKBX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    !wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Full-Economic-News-DFE-839861.csv\n",
        "    !ls -lah DATAPATH\n",
        "    data = pd.read_csv(\"DATAPATH/Full-Economic-News-DFE-839861.csv\" , encoding = \"ISO-8859-1\" )\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "    data = pd.read_csv(\"Data/Full-Economic-News-DFE-839861.csv\" , encoding = \"ISO-8859-1\" )"
      ],
      "metadata": {
        "id": "21PPrma-IbFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c92dd8-0d7f-4ba4-bb40-04406d7c9901"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-16 16:37:39--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Full-Economic-News-DFE-839861.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12383529 (12M) [text/plain]\n",
            "Saving to: ‘DATAPATH/Full-Economic-News-DFE-839861.csv’\n",
            "\n",
            "Full-Economic-News- 100%[===================>]  11.81M  38.0MB/s    in 0.3s    \n",
            "\n",
            "2024-02-16 16:37:40 (38.0 MB/s) - ‘DATAPATH/Full-Economic-News-DFE-839861.csv’ saved [12383529/12383529]\n",
            "\n",
            "total 12M\n",
            "drwxr-xr-x 2 root root 4.0K Feb 16 16:37 .\n",
            "drwxr-xr-x 1 root root 4.0K Feb 16 16:37 ..\n",
            "-rw-r--r-- 1 root root  12M Feb 16 16:37 Full-Economic-News-DFE-839861.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(data.shape) # Number of rows (instances) and columns in the dataset\n",
        "data[\"relevance\"].value_counts()/data.shape[0] # Class distribution in the dataset"
      ],
      "metadata": {
        "id": "ozzWbaycIgPC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "ffb1f87f-9c9a-442d-f1d5-fe5115489ed2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(8000, 15)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "no          0.821375\n",
              "yes         0.177500\n",
              "not sure    0.001125\n",
              "Name: relevance, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert label to a numerical variable\n",
        "data = data[data.relevance != \"not sure\"] # removing the data where we don't want relevance=\"not sure\".\n",
        "data.shape\n",
        "data['relevance'] = data.relevance.map({'yes':1, 'no':0}) # relevant is 1, not-relevant is 0.\n",
        "data = data[[\"text\",\"relevance\"]] # Let us take only the two columns we need.\n",
        "data.shape"
      ],
      "metadata": {
        "id": "7jUzICnrLd-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02884a6c-2054-475a-e84c-7af6661d7e3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7991, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BzpsCfCWL4IX",
        "outputId": "583cb558-4daa-4bd6-c127-0d606c7e459f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  relevance\n",
              "0  NEW YORK -- Yields on most certificates of dep...          1\n",
              "1  The Wall Street Journal Online</br></br>The Mo...          0\n",
              "2  WASHINGTON -- In an effort to achieve banking ...          0\n",
              "3  The statistics on the enormous costs of employ...          0\n",
              "4  NEW YORK -- Indecision marked the dollar's ton...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d72bc1b-f7db-4cfe-a2d4-4ddd8c4e2b3d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>relevance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The statistics on the enormous costs of employ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d72bc1b-f7db-4cfe-a2d4-4ddd8c4e2b3d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1d72bc1b-f7db-4cfe-a2d4-4ddd8c4e2b3d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1d72bc1b-f7db-4cfe-a2d4-4ddd8c4e2b3d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dd9fe738-f428-4557-ac05-8a6fb9792363\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd9fe738-f428-4557-ac05-8a6fb9792363')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dd9fe738-f428-4557-ac05-8a6fb9792363 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 7991,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"2\\t- billion - dollar drop in | gross national\\\" product for the! third quarter* was announced Nossiter</br></br>The decline itself was small. Total output of goods and services was estimated by the President\\u0089\\u00db\\u00aas Council of Economic Advisers at a yearly rate of $503 billion compared to $505 billion in the April-June period.</br></br>But a closer look at the figures\\u0089\\u00db\\u00d3which added up to the first output slump in two and a half years that couldn\\u0089\\u00db\\u00aat be traced to a strike\\u0089\\u00db\\u00d3increased the gloom.</br></br>After the long line of weaker business reports in September, virtually the last of the optimists are now found in the Administration\\u0089\\u00db\\u00aas inner circle.</br></br>Raymond J. Saulnier, chairman of the President\\u0089\\u00db\\u00aas Council, told a reporter yesterday, \\u0089\\u00db\\u00cfI do not see a basis in these figures for the recession talk that has become so widespread recently and which is, I fear, having some retarding effect on our economy.\\u0089\\u00db\\u009d \\u0089\\u00db\\u00a2 Real output of goods and services\\u0089\\u00db\\u00d3gross national product stripped of price inflation \\u0089\\u00db\\u00d3was lower in the third quarter than in the first or second. The yearly pace, expressed in constant, 1959 dollars went from $495.9 billion in the Jan-uary-March period to $497.4\",\n          \"NEW YORK -- The dollar advanced sharply against several key rivals Friday after the U.S. Labor Department reported that the April unemployment rate fell to 5.0% and 20,000 jobs were lost, far fewer than expected.</br></br>The euro declined by more than a full U.S. cent to $1.5360, its lowest level in more than a month. This comes a week after the currency climbed to a historical high of $1.6020. Also Friday, the dollar increased to its highest level since February against the yen.</br></br>U.S. employment fell in April for the fourth-straight month, but at a much slower pace than previously, suggesting the economy may be starting to find its footing after several months of stagnation. The dollar took this as positive news.</br></br>\\\"The underlying nonfarm number was way better than the average market expectations,\\\" especially as markets priced in room for even deeper disappointment just before the report's release, said Tom Fitzpatrick, global head of currency strategy at Citigroup in New York. The report \\\"will probably help the dollar in the short term.\\\"</br></br>The data included a surprising decline in the jobless rate and supported expectations that the Federal Reserve will keep official interest rates steady for an extended period as it gauges the effect of past rate cuts and recent credit initiatives on financial markets and the economy.\",\n          \"A \\u0089\\u00db\\u00f7\\u0089\\u00db\\u00f7controlled implosion\\u0089\\u00db\\u009d (top) begins to crumble the 16-story Carlton House Hotel in downtown Pittsburgh Saturday morning. The 28-vear-old building is being razed to make way for Renaissance II, the second phase of a major redevelopment of the city. Thick smoke billows from the building (center) as more than 1,000 explosive charges do their work. Seven seconds after it began, the demolition is completed (bottom). Construction of a 52-story office building has been proposed in place of the hotel, where former Soviet Premier Khrushchev stayed while visiting the city during his tour of the United States in 1959.</br></br>ROANOKE (UPI)\\u0089\\u00db\\u00d3Industry analysts believe recent labor unrest and other problems in Poland, South Africa and Australia may lead foreign coal buyers to depend more heavily on the United States than in recent years.</br></br>Poland\\u0089\\u00db\\u00aas labor changes, racial unrest in South Africa and a major miners\\u0089\\u00db\\u00aa strike in Australia\\u0089\\u00db\\u00d3the world\\u0089\\u00db\\u00aas three largest coal exporters behind the United States\\u0089\\u00db\\u00d3are spreading the he-' lief that the United States may be t he most stable source of coal, analysts' said.</br></br>\\u0089\\u00db\\u00cfThe United States\\u0089\\u00db\\u00aa image as a coal supplier has made a big comeback. Our problems are beginning to pale in com-parispn (with other coal exporters),\\u0089\\u00db\\u009d said Jack Kawa, a coal analyst with. Wheat, First Securities.</br></br>\\u0089\\u00db\\u00cfThe outlook for I he U.S. export is getting stronger and stronger as the world turns to coal. Customers are turning towards American coal,\\u0089\\u00db\\u009d ho said.\"\n        ],\n        \"num_unique_values\": 7985,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = data['text'].values.tolist()\n",
        "y = data['relevance'].values.tolist()"
      ],
      "metadata": {
        "id": "W6acNl8AIw6I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "mystopwords = set(stopwords.words(\"english\"))\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAjAFKilrXSK",
        "outputId": "d00b07de-ef7c-47d1-b7a7-8a77da1e2ec2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define text preprocessing steps\n",
        "lemmatized = []\n",
        "wn = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_corpus(texts):\n",
        "    def remove_stops_digits(tokens):\n",
        "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
        "        tokens_new = [wn.lemmatize(word) for word in tokens ]\n",
        "        return [token.lower() for token in tokens_new if token.lower() not in mystopwords and not token.isdigit()\n",
        "               and token not in punctuation]\n",
        "    #This return statement below uses the above function to process twitter tokenizer output further.\n",
        "    return [remove_stops_digits(word_tokenize(text)) for text in texts]"
      ],
      "metadata": {
        "id": "VM5bnncOra6L"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get preprossed data and take a look at some sample data\n",
        "X = preprocess_corpus(texts)\n",
        "print(len(y), len(X))\n",
        "print(X[1])\n",
        "print(y[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aESJ8zTrreKp",
        "outputId": "b4f59c8b-b295-42b8-f890-725131667046"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7991 7991\n",
            "['wall', 'street', 'journal', 'online', '/br', '/br', 'morning', 'brief', 'look', 'day', \"'s\", 'biggest', 'news', 'emailed', 'subscriber', 'a.m.', 'every', 'business', 'day', 'sign', 'e-mail', 'here.', '/br', '/br', 'friday', 'evening', 'congress', 'town', 'summer', 'recess', 'americans', 'heading', 'mid-august', 'weekend', 'bush', 'administration', 'sent', 'message', 'state', 'federal', 'government', 'make', 'tougher', 'national', 'child', \"'s\", 'insurance', 'program', 'cover', 'offspring', 'middle-income', 'families.', '/br', '/br', 'state', 'children', \"'s\", 'health', 'insurance', 'program', 'wa', 'created', 'help', 'child', 'whose', 'family', 'could', \"n't\", 'afford', 'insurance', \"n't\", 'qualify', 'medicaid', 'administration', 'official', 'tell', 'new', 'york', 'times', 'change', 'aimed', 'returning', 'program', 'low-', 'income', 'focus', 'assuring', \"n't\", 'become', 'replacement', 'private', 'insurance', 'administration', 'point', 'man', 'dennis', 'smith', 'wrote', 'state', 'official', 'saying', 'would', 'new', 'restriction', 'district', 'columbia', 'state', '--', 'including', 'california', 'new', 'york', '--', 'extend', 'plan', 'extend', 'coverage', 'child', 'whose', 'family', 'make', 'federal', 'poverty', 'level', 'family', 'three', '42,900', 'family', 'four', \"'s\", '51,625', 'new', 'limit', 'child', 'family', 'making', 'would', 'spend', 'one', 'year', 'uninsured', 'qualifying', 'state', 'want', 'extend', 'coverage', 'would', 'assure', 'washington', 'least', 'child', 'eligible', 'schip', 'medicaid', 'enrolled', 'one', 'program', 'associated', 'press', 'report', 'state', 'currently', 'make', 'assurances.', '/br', '/br', 'rachel', 'klein', 'deputy', 'director', 'health', 'policy', 'advocacy', 'group', 'families', 'usa', 'tell', 'ap', 'since', 'many', 'family', 'threshold', 'ca', \"n't\", 'afford', 'private', 'insurance', '``', 'effect', 'policy', 'uninsured', 'kid', \"''\", 'ann', 'clemency', 'kohler', 'deputy', 'commissioner', 'human', 'service', 'new', 'jersey', 'tell', 'times', 'change', '``', 'cause', 'havoc', 'program', 'could', 'jeopardize', 'coverage', 'thousand', 'child', \"''\", 'states', 'already', 'imposing', 'waiting', 'period', 'taking', 'step', 'prevent', 'parent', 'moving', 'child', 'private', 'insurance', 'schip', 'currently', 'serf', '6.6', 'million', 'child', 'washington', 'post', 'note', 'administration', \"'s\", 'new', 'restriction', 'come', 'program', 'expires', 'end', 'next', 'month', 'congress', 'doe', \"n't\", 'reauthorize', 'subject', 'larger', 'political', 'fight', 'pit', 'white', 'house', 'democrats', 'republicans', 'congress', 'state', 'capital']\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# further split the training data into training and test set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
      ],
      "metadata": {
        "id": "KS3Ux0cNkmCK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Tokenize the texts and convert them into word index vectors"
      ],
      "metadata": {
        "id": "65m3lyJaLTKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train) #Converting text to a vector of word indexes\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "val_sequences = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LFQ_RvoKDYi",
        "outputId": "27e09fee-58bc-4ee5-ef18-f15949539fce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 49414 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Pad the text sequences so that all text vectors are of the same length."
      ],
      "metadata": {
        "id": "LxZ-lKILLf5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
        "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
        "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "val_data = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "train_labels = to_categorical(np.asarray(y_train))\n",
        "test_labels = to_categorical(np.asarray(y_test))\n",
        "val_labels = to_categorical(np.asarray(y_val))"
      ],
      "metadata": {
        "id": "yO97jjrlLfE9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: If we want to use pre-trained embeddings to convert the train and test data\n",
        "into an embedding matrix like we did in the earlier examples with Word2vec and\n",
        "fastText, we have to download them and use them to convert our data into the input\n",
        "format for the neural networks. The following code snippet shows an example of how\n",
        "to do this using GloVe embeddings"
      ],
      "metadata": {
        "id": "vpl2bn3HLlbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "\n",
        "#Load pre trained glove model from Gensim\n",
        "w2v_model = gensim.downloader.load('glove-wiki-gigaword-100')"
      ],
      "metadata": {
        "id": "__aGKurx1_rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    if word in w2v_model:\n",
        "      embedding_vector = w2v_model[word]\n",
        "      if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "bHlqPm-SKGV_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Use the output from Step 3 as the input to a neural network architecture."
      ],
      "metadata": {
        "id": "NqBbrU5gL2LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ],
      "metadata": {
        "id": "CftmO6CrL1IL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41e0b2d-dc5a-47f4-807a-feb42980c6e3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing of embedding matrix is done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) 1D CNN model with training your own embedding"
      ],
      "metadata": {
        "id": "R__aIyL5KOZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(2, activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "#Train the model. Tune to validation set.\n",
        "cnnmodel.fit(train_data, train_labels,\n",
        "          batch_size=128,\n",
        "          epochs=1, validation_data=(val_data, val_labels))\n",
        "\n",
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn3iKIliKQbu",
        "outputId": "c6b14b3e-731d-43ed-dfd5-5c8e21de78f0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
            "38/38 [==============================] - 85s 2s/step - loss: 0.4899 - acc: 0.8054 - val_loss: 0.4526 - val_acc: 0.8310\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.4567 - acc: 0.8299\n",
            "Test accuracy with CNN: 0.8298937082290649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) 1D CNN Model with pre-trained embedding"
      ],
      "metadata": {
        "id": "XGq55jhVKIFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Define a 1D CNN model.')\n",
        "\n",
        "cnnmodel = Sequential()\n",
        "cnnmodel.add(embedding_layer)\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(MaxPooling1D(5))\n",
        "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "cnnmodel.add(GlobalMaxPooling1D())\n",
        "cnnmodel.add(Dense(128, activation='relu'))\n",
        "cnnmodel.add(Dense(2, activation='softmax'))\n",
        "\n",
        "cnnmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "#Train the model. Tune to validation set.\n",
        "cnnmodel.fit(train_data, train_labels,\n",
        "          batch_size=128,\n",
        "          epochs=1, validation_data=(val_data, val_labels))\n",
        "#Evaluate on test set:\n",
        "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "print('Test accuracy with CNN:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw4caQedKMt9",
        "outputId": "e2399c5d-ea4a-4c54-c79f-487975f1a26f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define a 1D CNN model.\n",
            "38/38 [==============================] - 60s 2s/step - loss: 0.5010 - acc: 0.8156 - val_loss: 0.4273 - val_acc: 0.8310\n",
            "50/50 [==============================] - 5s 101ms/step - loss: 0.4285 - acc: 0.8299\n",
            "Test accuracy with CNN: 0.8298937082290649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3) LSTM Model with training your own embedding"
      ],
      "metadata": {
        "id": "y_AmzAowKTBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
        "\n",
        "#model\n",
        "rnnmodel = Sequential()\n",
        "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel.add(Dense(2, activation='sigmoid'))\n",
        "rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Training the RNN')\n",
        "\n",
        "rnnmodel.fit(train_data, train_labels,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(val_data, val_labels))\n",
        "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1ShRzthKRod",
        "outputId": "f2e8c5cb-77ca-475d-87a1-c6c246dd166e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training an LSTM model, training embedding layer on the fly\n",
            "Training the RNN\n",
            "150/150 [==============================] - 500s 3s/step - loss: 0.4966 - accuracy: 0.7621 - val_loss: 0.4505 - val_accuracy: 0.7842\n",
            "63/63 [==============================] - 26s 404ms/step - loss: 0.4371 - accuracy: 0.7940\n",
            "Test accuracy with RNN: 0.7940000295639038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (4) LSTM Model using pre-trained Embedding Layer"
      ],
      "metadata": {
        "id": "37TovgmGKVjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
        "\n",
        "rnnmodel2 = Sequential()\n",
        "rnnmodel2.add(embedding_layer)\n",
        "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel2.add(Dense(2, activation='sigmoid'))\n",
        "rnnmodel2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Training the RNN')\n",
        "\n",
        "rnnmodel2.fit(train_data, train_labels,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(val_data, val_labels))\n",
        "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kik7FHGeKXjS",
        "outputId": "79b95860-c96f-4228-be25-bb14eb33e660"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining and training an LSTM model, using pre-trained embedding layer\n",
            "Training the RNN\n",
            "150/150 [==============================] - 452s 3s/step - loss: 0.4652 - accuracy: 0.8089 - val_loss: 0.5230 - val_accuracy: 0.8310\n",
            "50/50 [==============================] - 23s 452ms/step - loss: 0.5256 - accuracy: 0.8299\n",
            "Test accuracy with RNN: 0.8298937082290649\n"
          ]
        }
      ]
    }
  ]
}