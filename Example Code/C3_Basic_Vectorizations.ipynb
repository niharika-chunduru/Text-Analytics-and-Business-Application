{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Basic Vectorization Approaches"
      ],
      "metadata": {
        "id": "Pmm_OZQjo3Nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###One-Hot Encoding"
      ],
      "metadata": {
        "id": "cObTGAgepF-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF9kxbqgoG1b",
        "outputId": "be5abc4c-25c0-46bf-f160-18a58d162148"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the vocabulary\n",
        "vocab = {}\n",
        "count = 0\n",
        "for doc in processed_docs:\n",
        "    for word in doc.split():\n",
        "        if word not in vocab:\n",
        "            count = count +1\n",
        "            vocab[word] = count\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9XzAWO6oYTL",
        "outputId": "07e34425-5624-41f9-ca85-4f29259a8ee6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get one hot representation for any string based on this vocabulary.\n",
        "#If the word exists in the vocabulary, its representation is returned.\n",
        "#If not, a list of zeroes is returned for that word.\n",
        "def get_onehot_vector(somestring):\n",
        "    onehot_encoded = []\n",
        "    for word in somestring.split():\n",
        "        temp = [0]*len(vocab)\n",
        "        if word in vocab:\n",
        "            temp[vocab[word]-1] = 1 # -1 is to take care of the fact indexing in array starts from 0 and not 1\n",
        "        onehot_encoded.append(temp)\n",
        "    return onehot_encoded"
      ],
      "metadata": {
        "id": "rJnC3w8_ocF5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_docs[1])\n",
        "get_onehot_vector(processed_docs[1]) #one hot representation for a text from our corpus."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KErKZA7yodfy",
        "outputId": "70464fd5-ab57-456c-fa42-84adcfa86bbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man bites dog\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_onehot_vector(\"man and dog are good\")\n",
        "#one hot representation for a random text, using the above vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDLuefddoeSU",
        "outputId": "98f0a551-a7ba-4520-f3ee-4be5ba123455"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [1, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_onehot_vector(\"man and man are good\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXKu47tFofM_",
        "outputId": "17f28649-5e04-4533-d4f8-878c08033a4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###One-hot encoding using scikit -learn\n",
        "**We encode our corpus as a one-hot numeric array using scikit-learn's OneHotEncoder.**</br>\n",
        "We will demostrate:\n",
        "* One Hot Encoding: In one-hot encoding, each word w in corpus vocabulary is given a unique integer id wid that is between 1 and |V|, where V is the set of corpus vocab. Each word is then represented by a V-dimensional binary vector of 0s and 1s.\n",
        "\n",
        "* Label Encoding: In Label Encoding, each word w in our corpus is converted into a numeric value between 0 and n-1 (where n refers to number of unique words in our corpus)."
      ],
      "metadata": {
        "id": "nGC7l_KrpZ24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\n",
        "!pip install scikit-learn==0.21.3\n",
        "\n",
        "# ==========================="
      ],
      "metadata": {
        "id": "DtTrXl1TpP5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec2f2e9d-12d9-402a-980b-a4015a46cb92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.21.3\n",
            "  Downloading scikit-learn-0.21.3.tar.gz (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.21.3) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.21.3) (1.11.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.21.3) (1.3.2)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for scikit-learn\n",
            "Failed to build scikit-learn\n",
            "\u001b[31mERROR: Could not build wheels for scikit-learn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S1 = 'dog bites man'\n",
        "S2 = 'man bites dog'\n",
        "S3 = 'dog eats meat'\n",
        "S4 = 'man eats food'"
      ],
      "metadata": {
        "id": "pnNuv7byojLr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
        "values = data[0]+data[1]+data[2]+data[3]\n",
        "print(\"The data: \",values)\n",
        "\n",
        "#Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(\"Label Encoded:\",integer_encoded)\n",
        "\n",
        "#One-Hot Encoding\n",
        "onehot_encoder = OneHotEncoder()\n",
        "onehot_encoded = onehot_encoder.fit_transform(data).toarray()\n",
        "print(\"Onehot Encoded Matrix:\\n\",onehot_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KfOLDmSokDy",
        "outputId": "f24aa8ab-c45d-4fd8-9e46-c8258556a555"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data:  ['dog', 'bites', 'man', 'man', 'bites', 'dog', 'dog', 'eats', 'meat', 'man', 'eats', 'food']\n",
            "Label Encoded: [1 0 4 4 0 1 1 2 5 4 2 3]\n",
            "Onehot Encoded Matrix:\n",
            " [[1. 0. 1. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 1. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bag of Words"
      ],
      "metadata": {
        "id": "_ympYYC0onSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"] #Same as the earlier notebook\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvaXPwXIp9ig",
        "outputId": "e1726e5a-06ea-4d4c-c417-0faac3ebfa17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More information about CountVectorizer:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ],
      "metadata": {
        "id": "uXSxEuQI6Myh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#look at the documents list\n",
        "print(\"Our corpus: \", processed_docs)\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "#Build a BOW representation for the corpus\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "#Look at the vocabulary mapping\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "#see the BOW rep for first 2 documents\n",
        "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
        "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
        "\n",
        "#Get the representation using this vocabulary, for a new text\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lQyFRLqp-Es",
        "outputId": "3b4af731-adc8-4d5e-b045-cf403166d3db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
            "Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
            "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
            "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n",
            "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we represented the text considering the frequency of words into account. However, sometimes, we don't care about frequency much, but only want to know whether a word appeared in a text or not. That is, each document is represented as a vector of 0s and 1s. We will use the option binary=True in CountVectorizer for this purpose (It now becomes one-hot encoding)."
      ],
      "metadata": {
        "id": "ylTkigWqqAmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BoW with binary vectors\n",
        "count_vect = CountVectorizer(binary=True)\n",
        "count_vect.fit(processed_docs)\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgkhrsS5qB23",
        "outputId": "7fc865e7-4fb1-446d-ae9e-a4ed05e5f53b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bag of N-Grams\n",
        "One hot encoding, BoW and TF-IDF treat words as independent units. There is no notion of phrases or word ordering. Bag of Ngrams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n countigous words/tokens. This can help us capture some context, which earlier approaches could not do. Let us see how it works using the same toy corpus we used in earlier examples."
      ],
      "metadata": {
        "id": "Gp8PkP4rqGJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#our corpus\n",
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGcAVoUQqISx",
        "outputId": "6c2368f6-d0ae-463e-9874-fb33ec832fdf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer, which we used for BoW, can be used for getting a Bag of N-grams representation as well, using its ngram_range argument. The code snippet below shows how:"
      ],
      "metadata": {
        "id": "MO-uhvP1qUl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
        "count_vect = CountVectorizer(ngram_range=(1,3))\n",
        "\n",
        "#Build a BOW representation for the corpus\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "#Look at the vocabulary mapping\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "#see the BOW rep for first 2 documents\n",
        "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
        "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
        "\n",
        "#Get the representation using this vocabulary, for a new text\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ids1a6FvqSqY",
        "outputId": "efed4153-80b2-453c-f39c-28d0fe3e07ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
            "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
            "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
            "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note that the number of features (and hence the size of the feature vector) increased a lot for the same data, compared to the ther single word based representations!!**"
      ],
      "metadata": {
        "id": "C40jQzE_qY3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF\n",
        "In all the other approaches we saw so far, all the words in the text are treated equally important. There is no notion of some words in the document being more important than others. TF-IDF addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus. It was commonly used representation scheme for information retrieval systems, for extracting relevant documents from a corpus for given text query."
      ],
      "metadata": {
        "id": "RKgMBQEjqcu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "processed_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frxje3qsqXA4",
        "outputId": "7b492987-a54e-425e-8fb3-2e3a34ed71b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More information about TfidfVectorizer:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ],
      "metadata": {
        "id": "X6b4aHRC6e_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Want to know how TfidfVectorizer calcualte the Tfidf values? Find from below references:\n",
        "\n",
        "https://stackoverflow.com/questions/36966019/how-aretf-idf-calculated-by-the-scikit-learn-tfidfvectorizer\n",
        "\n",
        "https://github.com/scikit-learn/scikit-learn/blob/1.3.2/sklearn/feature_extraction/text.py#L1671"
      ],
      "metadata": {
        "id": "ng78V5fiCPT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "print(\"The processed docs\")\n",
        "print(processed_docs)\n",
        "print(\"-\"*10)\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
        "\n",
        "#All words in the vocabulary.\n",
        "print(\"All words in the vocabulary\",tfidf.get_feature_names_out())\n",
        "print(\"-\"*10)\n",
        "\n",
        "#IDF for all words in the vocabulary\n",
        "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
        "print(\"-\"*10)\n",
        "\n",
        "#TFIDF representation for all documents in our corpus\n",
        "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray())\n",
        "print(\"-\"*10)\n",
        "\n",
        "temp = tfidf.transform([\"dog and man are friends\"])\n",
        "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHvL85m2qkqQ",
        "outputId": "3bcd56d4-4c8a-478a-f58a-9b0ac91b37e3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The processed docs\n",
            "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
            "----------\n",
            "All words in the vocabulary ['bites' 'dog' 'eats' 'food' 'man' 'meat']\n",
            "----------\n",
            "----------\n",
            "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
            "----------\n",
            "TFIDF representation for all documents in our corpus\n",
            " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
            " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
            "----------\n",
            "Tfidf representation for 'dog and man are friends':\n",
            " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
          ]
        }
      ]
    }
  ]
}